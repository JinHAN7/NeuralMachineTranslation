{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import pickle as pkl\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import sacrebleu\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "# get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import math\n",
    "from sacrebleu import raw_corpus_bleu, corpus_bleu\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device\n",
    "#print('VI_preidx_RNN_attn_lr1e3_hidden512_TF.py')\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "'''old version of data loaders. build the lang without pretrained index'''\n",
    "# SOS_token = 0\n",
    "# EOS_token = 1\n",
    "# PAD_IDX = 2\n",
    "# UNK_IDX = 3\n",
    "# class Lang:\n",
    "#     def __init__(self, name):\n",
    "#         self.name = name\n",
    "#         self.word2index = {}\n",
    "#         self.word2count = {}\n",
    "#         self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"PAD\", 3:\"UNK\"}\n",
    "#         self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "#     def addSentence(self, sentence):\n",
    "#         for word in sentence:\n",
    "#             self.addWord(word)\n",
    "\n",
    "#     def addWord(self, word):\n",
    "#         if word not in self.word2index:\n",
    "#             self.word2index[word] = self.n_words\n",
    "#             self.word2count[word] = 1\n",
    "#             self.index2word[self.n_words] = word\n",
    "#             self.n_words += 1\n",
    "#         else:\n",
    "#             self.word2count[word] += 1\n",
    "\n",
    "            \n",
    "# def loadingLangs(sourcelang, targetlang, setname):\n",
    "#     input_ls = []\n",
    "#     output_ls = []\n",
    "#     print('Reading lines...')\n",
    "#     # Read the file \n",
    "#     with open('/home/jh5695/nlp_project/nlp_data/iwslt-%s-%s/%s.tok.%s'%(sourcelang, targetlang, setname,sourcelang)) as f:\n",
    "#         for line in f.readlines():\n",
    "#             input_ls.append([normalizeString(word) for word in line.split()])\n",
    "#     with open('/home/jh5695/nlp_project/nlp_data/iwslt-%s-%s/%s.tok.%s'%(sourcelang, targetlang, setname,targetlang)) as f:\n",
    "#         for line in f.readlines():\n",
    "#             output_ls.append([normalizeString(word) for word in line.split()])\n",
    "#     pairs = list(zip(input_ls, output_ls))\n",
    "#     print('Read %s sentence pairs'%(len(input_ls)))\n",
    "#     input_lang = Lang(sourcelang)\n",
    "#     output_lang = Lang(targetlang)\n",
    "#     print(\"Counting words...\")\n",
    "#     for pair in pairs:\n",
    "#         input_lang.addSentence(pair[0])\n",
    "#         output_lang.addSentence(pair[1])\n",
    "#     print(\"Counted words:\")\n",
    "#     print(input_lang.name, input_lang.n_words)\n",
    "#     print(output_lang.name, output_lang.n_words)\n",
    "#     return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "data_dir = '/home/jh5695/nlp_project/nlp_data'\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_IDX = 2\n",
    "UNK_IDX = 3\n",
    "VOCAB_SIZE = 50000\n",
    "\n",
    "\n",
    "def pkl_loader(file_name):\n",
    "    with open(file_name+'.p', 'rb') as f:\n",
    "        objct = pkl.load(f)\n",
    "    return(objct)\n",
    "\n",
    "def pkl_dumper(obj, file_name):\n",
    "    with open(file_name+'.p', 'wb') as f:\n",
    "        pkl.dump(obj, f, protocol=None)\n",
    "\n",
    "def load_pretrained_wordvec(lan):\n",
    "    if lan == 'zh':\n",
    "        filename = 'wiki.zh.vec'\n",
    "    if lan == 'en':\n",
    "        filename = 'wiki-news-300d-1M.vec'\n",
    "    if lan == 'vi':\n",
    "        filename = 'wiki.vi.vec'\n",
    "        \n",
    "    with open(os.path.join(data_dir, filename),encoding='utf-8') as f:\n",
    "        word_vecs = np.zeros((VOCAB_SIZE+4, 300))\n",
    "        word_vecs[UNK_IDX] = np.random.normal(scale=0.6, size=(300, ))\n",
    "        word_vecs[SOS_token] = np.random.normal(scale=0.6, size=(300, ))\n",
    "        word_vecs[EOS_token] = np.random.normal(scale=0.6, size=(300, ))\n",
    "\n",
    "        words_ft = {'<pad>': PAD_IDX,\n",
    "                   '<unk>': UNK_IDX, \n",
    "                   '<sos>': SOS_token,\n",
    "                   '<eos>': EOS_token}\n",
    "        idx2words_ft = {PAD_IDX:'<pad>', UNK_IDX: '<unk>', SOS_token: '<sos>', EOS_token: '<eos>'}\n",
    "        ordered_words_ft = ['<sos>', '<eos>', '<pad>', '<unk>']\n",
    "        count = 0\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            if len(idx2words_ft) >= VOCAB_SIZE: \n",
    "                break\n",
    "            s = line.split()\n",
    "            if (np.asarray(s[1:]).size != 300):\n",
    "                print(lan, i, np.asarray(s[1:]).size)\n",
    "                continue\n",
    "            word_vecs[count+4, :] = np.asarray(s[1:])\n",
    "            words_ft[s[0]] = count+4\n",
    "            idx2words_ft[count+4] = s[0]\n",
    "            ordered_words_ft.append(s[0])\n",
    "            count += 1\n",
    "    word_vecs = torch.FloatTensor(word_vecs)\n",
    "    pkl_dumper(word_vecs, os.path.join(data_dir, lan + '_word_vecs'))\n",
    "    pkl_dumper(words_ft, os.path.join(data_dir, lan + '_words_ft'))\n",
    "    pkl_dumper(idx2words_ft, os.path.join(data_dir, lan + '_idx2words_ft'))\n",
    "    pkl_dumper(ordered_words_ft, os.path.join(data_dir, lan + '_ordered_words_ft'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "load_pretrained_wordvec('vi')\n",
    "load_pretrained_wordvec('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[4]:\n",
    "word_vecs = {}\n",
    "word2index = {}\n",
    "index2word = {}\n",
    "word_vecs['en'] = pkl_loader('/home/jh5695/nlp_project/nlp_data/en_word_vecs')\n",
    "word_vecs['vi'] = pkl_loader('/home/jh5695/nlp_project/nlp_data/vi_word_vecs')\n",
    "word2index['en'] = pkl_loader('/home/jh5695/nlp_project/nlp_data/en_words_ft')\n",
    "word2index['vi'] = pkl_loader('/home/jh5695/nlp_project/nlp_data/vi_words_ft')\n",
    "index2word['en'] = pkl_loader('/home/jh5695/nlp_project/nlp_data/en_idx2words_ft')\n",
    "index2word['vi'] = pkl_loader('/home/jh5695/nlp_project/nlp_data/vi_idx2words_ft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(word2index['en'])\n",
    "VOCAB_SIZE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name, index2word, word2index):\n",
    "        self.name = name\n",
    "        self.word2index = word2index\n",
    "        self.index2word = index2word\n",
    "        self.n_words = len(index2word)\n",
    "\n",
    "\n",
    "\n",
    "import html\n",
    "def normalizeString(s):\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = html.unescape(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.5)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def loadingLangs(sourcelang, targetlang, setname):\n",
    "    input_ls = []\n",
    "    output_ls = []\n",
    "    print('Reading lines...')\n",
    "    # Read the file \n",
    "    with open(data_dir+'/iwslt-%s-%s/%s.tok.%s'%(sourcelang, targetlang, setname,sourcelang), encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            input_ls.append([normalizeString(word) for word in line.split()])\n",
    "    with open(data_dir+'/iwslt-%s-%s/%s.tok.%s'%(sourcelang, targetlang, setname,targetlang), encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            output_ls.append([normalizeString(word) for word in line.split()])\n",
    "    pairs = list(zip(input_ls, output_ls))\n",
    "    pairs = [pair for pair in pairs if (len(pair[0])+len(pair[1]))>0]\n",
    "    print('Read %s sentence pairs'%(len(input_ls)))\n",
    "    if sourcelang == 'zh':\n",
    "        input_lang = Lang(sourcelang, index2word['zh'], word2index['zh'])\n",
    "    if sourcelang =='vi':\n",
    "        input_lang = Lang(sourcelang, index2word['vi'], word2index['vi'])\n",
    "    output_lang = Lang(targetlang, index2word['en'], word2index['en'])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 133317 sentence pairs\n",
      "Counted words:\n",
      "vi 50000\n",
      "en 50000\n",
      "Reading lines...\n",
      "Read 1268 sentence pairs\n",
      "Counted words:\n",
      "vi 50000\n",
      "en 50000\n",
      "Reading lines...\n",
      "Read 1553 sentence pairs\n",
      "Counted words:\n",
      "vi 50000\n",
      "en 50000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "source_tra, target_tra, pairs_tra = loadingLangs('vi', 'en', 'train')\n",
    "source_val, target_val, pairs_val = loadingLangs('vi', 'en', 'dev')\n",
    "source_tes, target_tes, pairs_tes = loadingLangs('vi', 'en', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_pre_trained_emb = word_vecs['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pre_trained_emb = word_vecs['vi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "# print(\"90% of vit sentences length = {0}\".format(np.percentile([len(x[0]) for x in pairs_tra], 90)))\n",
    "# print(\"90% of english sentences length = {0}\".format(np.percentile([len(x[1]) for x in pairs_tra], 90)))\n",
    "# print(random.choice(pairs_tra))\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "MAX_SENT_LEN = 38\n",
    "BATCH_SIZE = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] if word in lang.word2index else UNK_IDX for word in sentence]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair,source,target):\n",
    "    input_lang = source\n",
    "    output_lang = target\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0]).reshape((-1))\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1]).reshape((-1))\n",
    "    return (input_tensor, input_tensor.shape[0], target_tensor, target_tensor.shape[0])\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, source, target, pairs):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.pairs = pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        inp_ten, inp_len, tar_ten, tar_len = tensorsFromPair(self.pairs[key], self.source, self.target)\n",
    "        item = {}\n",
    "        item['inputtensor'] = inp_ten[:MAX_SENT_LEN]\n",
    "        item['inputlen'] = min(inp_len, MAX_SENT_LEN)\n",
    "        item['targettensor'] = tar_ten[:MAX_SENT_LEN]\n",
    "        item['targetlen'] = min(tar_len, MAX_SENT_LEN)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = NMTDataset(source_tra, target_tra, pairs_tra)\n",
    "val_data = NMTDataset(source_tra, target_tra, pairs_val)\n",
    "test_data = NMTDataset(source_tra, target_tra, pairs_tes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Dataloader\n",
    "#collate function\n",
    "\n",
    "def collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    src_data, tar_data, src_len, tar_len = [], [], [], []\n",
    "    for datum in batch:        \n",
    "        src_datum = np.pad(np.array(datum['inputtensor']),\n",
    "                                pad_width=((0,MAX_SENT_LEN-datum['inputlen'])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        tar_datum = np.pad(np.array(datum['targettensor']),\n",
    "                                pad_width=((0,MAX_SENT_LEN-datum['targetlen'])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        src_data.append(src_datum)\n",
    "        tar_data.append(tar_datum)\n",
    "        src_len.append(datum['inputlen'])\n",
    "        tar_len.append(datum['targetlen'])\n",
    "        \n",
    "    ind_dec_order = np.argsort(src_len)[::-1]\n",
    "    src_data = np.array(src_data)[ind_dec_order]\n",
    "    src_len = np.array(src_len)[ind_dec_order]\n",
    "    tar_data = np.array(tar_data)[ind_dec_order]\n",
    "    tar_len = np.array(tar_len)[ind_dec_order]\n",
    "    return [torch.from_numpy(np.array(src_data)).to(device),torch.from_numpy(np.array(tar_data)).to(device),\n",
    "                torch.LongTensor(np.array(src_len)).to(device),torch.LongTensor(np.array(tar_len)).to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=BATCH_SIZE,shuffle=True, collate_fn=collate_func)\n",
    "val_loader = torch.utils.data.DataLoader(val_data,\n",
    "                                           batch_size=BATCH_SIZE,shuffle=False, collate_fn=collate_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper = {\n",
    "    'HIDDEN_SIZE': 512,\n",
    "    'LR': 0.0001,\n",
    "    'EVA_EVERY': 100,\n",
    "    'DROP_OUT': 0,\n",
    "    'TEACHER_RATIO': 1,\n",
    "    'N_LAYERS': 1,\n",
    "    'KER_SIZE': 3,\n",
    "    'NUM_EPOCHS': 30   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, pretra = False,n_layers=1, dropout=0.5):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        if pretra:\n",
    "            emb_mat = torch.from_numpy(source_pre_trained_emb.numpy()).float()\n",
    "            self.embedding = nn.Embedding.from_pretrained(emb_mat,freeze = True)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(input_size,embed_size,padding_idx = PAD_IDX)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        '''\n",
    "        :param input_seqs: \n",
    "            Variable of shape (num_step(T),batch_size(B)), sorted decreasingly by lengths(for packing)\n",
    "        :param input:\n",
    "            list of sequence length\n",
    "        :param hidden:\n",
    "            initial state of GRU\n",
    "        :returns:\n",
    "            GRU outputs in shape (T,B,hidden_size(H))\n",
    "            last hidden stat of RNN(i.e. last output for GRU)\n",
    "        '''\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)  # unpack (back to padded)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]  # Sum bidirectional outputs\n",
    "#         print('enc, output size', outputs.size())\n",
    "#         print('enc, hidden size', hidden.size())\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "#Some code was inspired by Bahdanau's work.\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.normal_(mean=0, std=stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        max_len = encoder_outputs.size(0)\n",
    "        Hid_copy = hidden.repeat(max_len,1,1).transpose(0,1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0,1) \n",
    "        attn_energies = self.score(Hid_copy,encoder_outputs) \n",
    "        return F.softmax(attn_energies).unsqueeze(1) \n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        score = F.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n",
    "        score = score.transpose(2,1) \n",
    "        v = self.v.repeat(encoder_outputs.data.shape[0],1).unsqueeze(1) \n",
    "        score = torch.bmm(v,score) \n",
    "        return score.squeeze(1) \n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hid_size, emb_size, out_size, pretra = False,n_layers=1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "       \n",
    "        self.n_layers = n_layers\n",
    "        self.hid_size = hid_size\n",
    "        self.emb_size = emb_size\n",
    "        self.out_size = out_size\n",
    "          \n",
    "        if pretra:\n",
    "            emb_mat = torch.from_numpy(target_pre_trained_emb.numpy()).float()\n",
    "            self.embedding = nn.Embedding.from_pretrained(emb_mat,freeze = True)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(out_size, emb_size)\n",
    "       \n",
    "        self.attn = Attn('concat', hid_size)\n",
    "        self.gru = nn.GRU(hid_size + emb_size, hidden_size, n_layers, dropout=0.1)\n",
    "      \n",
    "        self.out = nn.Linear(hid_size, out_size)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        \n",
    "        \n",
    "#         print('dec, word input', word_input.size())\n",
    "        word_embedded = self.embedding(word_input.squeeze()).view(1, -1, self.emb_size) \n",
    "       \n",
    "#         print('dec, wembd', word_embedded.size())\n",
    "        \n",
    "#         print('dec, last hidden', last_hidden.size())\n",
    "#         print('dec, enc outp', encoder_outputs.size())\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  \n",
    "        context = context.transpose(0, 1) \n",
    "#         print('dec, context', context.size())\n",
    "#         print('dec, wembd', word_embedded.size())\n",
    "        \n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "#         print('dec rnn input', rnn_input.size())\n",
    "#         print('dec last hidden', last_hidden.size())\n",
    "        \n",
    "        output, hidden = self.gru(rnn_input, last_hidden[-1].unsqueeze(0))\n",
    "        output = output.squeeze(0)  \n",
    "        \n",
    "        output = F.log_softmax(self.out(output))\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, input_len, target_len, encoder, decoder,\n",
    "          encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_SENT_LEN):\n",
    "    \n",
    "    hidden_size = hyper['HIDDEN_SIZE']\n",
    "    learning_rate = hyper['LR']\n",
    "    dropout_p = hyper['DROP_OUT']\n",
    "    teacher_forcing_ratio = hyper['TEACHER_RATIO']\n",
    "    n_layers = hyper['N_LAYERS']\n",
    "    ker_size = hyper['KER_SIZE']\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    scheduler_encoder = ExponentialLR(encoder_optimizer, 0.95, last_epoch=-1) \n",
    "    scheduler_decoder = ExponentialLR(decoder_optimizer, 0.95, last_epoch=-1) \n",
    "\n",
    "    max_input_len = max(input_len)\n",
    "    max_target_len = max(target_len)\n",
    "    \n",
    "    batch_size = input_tensor.size()[1]\n",
    "    \n",
    "#     encoder_hidden = encoder.initHidden(batch_size)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # feed-forward layer resulting encoder outputs, ei refers to each word token in input sentence\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor, input_len)\n",
    "    decoder_input = torch.tensor([[SOS_token]*batch_size], device=device) \n",
    "    \n",
    "#     decoder_hidden = nn.Linear(2*hidden_size,hidden_size)(\n",
    "#             torch.cat((encoder_hidden[0].cpu(),encoder_hidden[1].cpu()),dim = 1)).to(device).unsqueeze(0).to(device)\n",
    "   # print('encoder_hidden size:', encoder_hidden.size())\n",
    "    decoder_hidden = encoder_hidden#torch.cat([encoder_hidden[0, :, :].unsqueeze(0), encoder_hidden[1, :, :].unsqueeze(0)], dim = 0)\n",
    "\n",
    "     \n",
    "    #print('input to decoder:', decoder_input.size(), decoder_hidden.size(), encoder_outputs.size())\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing: \n",
    "        for di in range(max_target_len):\n",
    "            #if mode_dec == 'attn':\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "                \n",
    "            #else:\n",
    "                #decoder_output, decoder_hidden = decoder(\n",
    "                    #decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_input = target_tensor[di].view(batch_size,1)  \n",
    "            loss += criterion(decoder_output, target_tensor[di])    \n",
    "    else:\n",
    "  \n",
    "        for di in range(max_target_len):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().unsqueeze(1)\n",
    "#             torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=5)\n",
    "#             torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=5)\n",
    "#             print('loss dec', decoder_output.size())\n",
    "#             print('loss target tensor', target_tensor.size())\n",
    "    \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), 3)\n",
    "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), 3)\n",
    "    encoder_optimizer.step() \n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / float(max_target_len)\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "def bleu_new(corpus,truths):\n",
    "    n = len(corpus)\n",
    "    #bleu = [0]*n\n",
    "    pred_ls = []\n",
    "    true_ls = []\n",
    "    for i in range(n):\n",
    "        pred, true = corpus[i], truths[i]\n",
    "        pred_ls.append( [convert_idx_2_sent_new(sent, target_tra) for sent in pred])\n",
    "        true_ls.append([convert_idx_2_sent_new(sent, target_tra) for sent in true])\n",
    "    flattened_pred  = [val for sublist in pred_ls for val in sublist]\n",
    "    flattened_true  = [val for sublist in true_ls for val in sublist]\n",
    "    bleu= corpus_bleu(flattened_pred, [flattened_true]).score\n",
    "    return bleu\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "\n",
    "def convert_idx_2_sent_new(idx_tensor, lang_obj):\n",
    "    word_list = []\n",
    "    #truth_word_list = []\n",
    "    for i in idx_tensor:\n",
    "        if i.item() not in set([PAD_IDX,EOS_token,SOS_token]):\n",
    "            word_list.append(lang_obj.index2word[i.item()])\n",
    "#     for j in truth_tensor:\n",
    "#         if j.item() not in set([PAD_IDX,EOS_token,SOS_token]):\n",
    "#             truth_word_list.append(lang_obj.index2word[j.item()])\n",
    "    sent = (' ').join(word_list)\n",
    "    #truth_sent = (' ').join(truth_word_list)\n",
    "    return sent\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "def evaluate(encoder, decoder, data_loader, mode_enc, mode_dec, max_length=MAX_SENT_LEN):\n",
    "    start = time.time()\n",
    "    hidden_size = hyper['HIDDEN_SIZE']\n",
    "    learning_rate = hyper['LR']\n",
    "    dropout_p = hyper['DROP_OUT']\n",
    "    teacher_forcing_ratio = hyper['TEACHER_RATIO']\n",
    "    n_layers = hyper['N_LAYERS']\n",
    "    ker_size = hyper['KER_SIZE']\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    inputs = []\n",
    "    corpus = []\n",
    "    truths = []\n",
    "    for i, (input_sentences, target_sentences,len1,len2) in enumerate(data_loader):\n",
    "#         if i % 5 == 0:\n",
    "#             print('Time: {}, Step: [{}/{}]'.format(\n",
    "#                 timeSince(start, i + 1/len(train_loader)), i, len(data_loader)))\n",
    "        \n",
    "        input_tensor = input_sentences.transpose(0,1).to(device) #L*B\n",
    "        target_tensor = target_sentences.transpose(0,1).to(device)\n",
    "    \n",
    "        inputs.append(input_sentences)\n",
    "        truths.append(target_sentences)\n",
    "        \n",
    "        max_input_len = max(len1)\n",
    "        max_target_len = max(len2)\n",
    "        \n",
    "        \n",
    "        batch_size = input_tensor.size()[1]\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor, len1)\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]*batch_size], device=device) \n",
    "        \n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_words = torch.zeros(batch_size, int(max_input_len*1.5))\n",
    "    \n",
    "        for di in range(int(max_input_len*1.5)):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)        \n",
    "            topv, topi = decoder_output.topk(1) #topi B*1\n",
    "            decoded_words[:,di] = topi.squeeze()\n",
    "            decoder_input = topi.squeeze().detach().unsqueeze(1)\n",
    "            \n",
    "        corpus.append(decoded_words)\n",
    "#         print(inputs[0].size(), corpus[0].size(), truths[0].size()) # all B*L\n",
    "    return inputs, corpus, truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_tra.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50004, 300])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_pre_trained_emb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50004, 300])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_pre_trained_emb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CuDNN error: CUDNN_STATUS_EXECUTION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-0a29b3a95fea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         loss = train(input_tensor, target_tensor, len1, len2, encoder,\n\u001b[0;32m---> 53\u001b[0;31m                    decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mloss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-b6c8738cac1c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, input_len, target_len, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;31m# feed-forward layer resulting encoder outputs, ei refers to each word token in input sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSOS_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-b6c8738cac1c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seqs, input_lengths, hidden)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# unpack (back to padded)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Sum bidirectional outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hx, batch_sizes)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0mdropout_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_dropout_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mweight_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/backends/cudnn/rnn.py\u001b[0m in \u001b[0;36minit_dropout_state\u001b[0;34m(dropout, train, dropout_seed, dropout_state)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mdropout_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mself_ty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 device=torch.device('cuda')))\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mdropout_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdropout_desc_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdropout_ts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CuDNN error: CUDNN_STATUS_EXECUTION_FAILED"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "hidden_size = hyper['HIDDEN_SIZE']\n",
    "learning_rate = hyper['LR']\n",
    "eva_every = 1#hyper['EVA_EVERY']\n",
    "dropout_p = hyper['DROP_OUT']\n",
    "teacher_forcing_ratio = 1 #hyper['TEACHER_RATIO']\n",
    "n_layers = hyper['N_LAYERS']\n",
    "ker_size = hyper['KER_SIZE']\n",
    "num_epoch = hyper['NUM_EPOCHS']\n",
    "early_stopping = False\n",
    "patience = 3\n",
    "required_progress = 0.01\n",
    "\n",
    "# input_size, embed_size, hidden_size,\n",
    "encoder = EncoderRNN(source_tra.n_words, 300, hidden_size,False).to(device)\n",
    "\n",
    "#     else:\n",
    "#         encoder = EncoderCNN(source_tra.n_words, hidden_size, word_vecs=word_vecs[source_tra.name], dropout_p=dropout_p, ker_size=ker_size).to(device)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "\n",
    "#     if mode_dec == 'attn':\n",
    "#             decoder = AttnDecoder(hidden_size, target_tra.n_words, dropout_p=dropout_p).to(device)\n",
    "#     else:\n",
    "decoder = AttnDecoderRNN(hidden_size, 300, target_tra.n_words,False, n_layers=1).to(device)\n",
    "\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss() \n",
    "plot_bleu_score_val = []\n",
    "plot_losses = []\n",
    "loss_total = 0 \n",
    "best_score = None\n",
    "count = 0\n",
    "filename = 'VI_best_rnnattn_preidx_lr1e3_hidden512_TF' #########\n",
    "whole_val_bleu = []\n",
    "whole_train_loss = []\n",
    "for epoch in range(1, num_epoch + 1): \n",
    "    plot_bleu_score_val = []\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    for i, (input_sentences, target_sentences,len1,len2) in enumerate(train_loader): \n",
    "      ### delete break\n",
    "\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        input_tensor = input_sentences.transpose(0,1).to(device)    \n",
    "        target_tensor = target_sentences.transpose(0,1).to(device)\n",
    "        loss = train(input_tensor, target_tensor, len1, len2, encoder,\n",
    "                   decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        loss_total += loss\n",
    "        \n",
    "        if i > 0 and i % hyper['EVA_EVERY'] == 0:\n",
    "            inputs, corpus, truths = evaluate(encoder, decoder, val_loader, max_length=MAX_SENT_LEN, mode_enc='rnn', mode_dec='noattn')\n",
    "#              bleu_score_val = bleu_new(corpus, truths)\n",
    "            bleu_score_val_avg = bleu_new(corpus, truths)#np.mean(bleu_score_val)\n",
    "            loss_avg = loss_total / eva_every\n",
    "            loss_total = 0\n",
    "            plot_losses.append(loss_avg)\n",
    "            plot_bleu_score_val.append(bleu_score_val_avg)\n",
    "            if best_score is None:\n",
    "                best_score = bleu_score_val_avg\n",
    "            if bleu_score_val_avg < best_score + required_progress:\n",
    "                count += 1\n",
    "            elif bleu_score_val_avg > best_score:\n",
    "                state = {'epoch': epoch + 1, \n",
    "                           'state_dict_enc': encoder.state_dict(),\n",
    "                           'state_dict_dec': decoder.state_dict(), \n",
    "                           'best_accuracy': best_score, \n",
    "                           'optimizer_enc': encoder_optimizer.state_dict(),\n",
    "                          'optimizer_dec': decoder_optimizer.state_dict()}\n",
    "                print ('new best achieved')\n",
    "                torch.save(state, filename+'.pth')\n",
    "                pkl_dumper(inputs, os.path.join(data_dir, 'VI_preidx_attn_lr1e3_hidden512_inputs_TF'))\n",
    "                pkl_dumper(corpus,os.path.join(data_dir, 'VI_preidx_attn_lr1e3_hidden512_pred_corpus_TF'))\n",
    "                pkl_dumper(truths, os.path.join(data_dir,'VI_preidx_attn_lr1e3_hidden512_target_truths_TF'))\n",
    "                #  \n",
    "                best_score = bleu_score_val_avg\n",
    "                #count = 0\n",
    "#             if early_stopping:\n",
    "#                 if count >= patience:\n",
    "#                     print(\"earily stop triggered\")\n",
    "#                     break\n",
    "            print('-----------------------------------------')\n",
    "            print('Time: {0}, Epoch: [{1}/{2}], Step: [{3}/{4}], Train Loss: {5}, BLEU score: {6}'.format(\n",
    "                  timeSince(start, i + 1/len(train_loader)), epoch, num_epoch, i, \n",
    "                  len(train_loader), loss_avg, bleu_score_val_avg))\n",
    "            print('\\nInput> %s'%(' '.join([source_tra.index2word[i.item()] for i in inputs[0][3] if i.item() not in set([PAD_IDX,EOS_token,SOS_token])])))\n",
    "            print('\\nTarget= %s'%(convert_idx_2_sent_new(truths[0][3], target_tra)),\n",
    "              '\\nPredict< %s' %(convert_idx_2_sent_new(corpus[0][3], target_tra)))\n",
    "\n",
    "            print('\\nInput2> %s'%(' '.join([source_tra.index2word[i.item()] for i in inputs[0][13] if i.item() not in set([PAD_IDX,EOS_token,SOS_token])])))\n",
    "            print('\\nTarget2= %s'%(convert_idx_2_sent_new(truths[0][13], target_tra)),\n",
    "              '\\nPredict2< %s' %(convert_idx_2_sent_new(corpus[0][13], target_tra)))\n",
    "    whole_train_loss.append(plot_losses)\n",
    "    whole_val_bleu.append(plot_bleu_score_val)\n",
    "#             print('-----------------------------------------')\n",
    "    pkl_dumper(whole_train_loss, os.path.join(data_dir, 'VI_preidx_attn_train_lr1e3_hidden512_loss_TF'))\n",
    "    pkl_dumper(whole_val_bleu,os.path.join(data_dir, 'VI_preidx_attn_val_lr1e3_hidden512_bleu_TF'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
